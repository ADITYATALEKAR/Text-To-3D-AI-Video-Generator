{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c0ecfcd-c575-4feb-8155-ab94f916a636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting diffusers\n",
      "  Using cached diffusers-0.35.1-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.11/site-packages (2.3.1)\n",
      "Requirement already satisfied: torchvision in /opt/conda/lib/python3.11/site-packages (0.18.1)\n",
      "Requirement already satisfied: opencv-python in /opt/conda/lib/python3.11/site-packages (4.10.0.84)\n",
      "Requirement already satisfied: pillow in /opt/conda/lib/python3.11/site-packages (10.4.0)\n",
      "Collecting imageio\n",
      "  Using cached imageio-2.37.0-py3-none-any.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.11/site-packages (5.9.8)\n",
      "Requirement already satisfied: importlib_metadata in /opt/conda/lib/python3.11/site-packages (from diffusers) (7.0.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from diffusers) (3.15.4)\n",
      "Requirement already satisfied: huggingface-hub>=0.34.0 in /opt/conda/lib/python3.11/site-packages (from diffusers) (0.34.4)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.11/site-packages (from diffusers) (1.26.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.11/site-packages (from diffusers) (2024.5.15)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from diffusers) (2.31.0)\n",
      "Collecting safetensors>=0.3.1 (from diffusers)\n",
      "  Using cached safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.11/site-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.11/site-packages (from torch) (1.12.1)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.11/site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /opt/conda/lib/python3.11/site-packages (from torch) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.11/site-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.11/site-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.11/site-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/lib/python3.11/site-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/lib/python3.11/site-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /opt/conda/lib/python3.11/site-packages (from torch) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: triton==2.3.1 in /opt/conda/lib/python3.11/site-packages (from torch) (2.3.1)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.5.82)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.34.0->diffusers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.34.0->diffusers) (6.0.1)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.34.0->diffusers) (4.66.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.34.0->diffusers) (1.1.8)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.11/site-packages (from importlib_metadata->diffusers) (3.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch) (2.1.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->diffusers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->diffusers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->diffusers) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->diffusers) (2023.11.17)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n",
      "Using cached diffusers-0.35.1-py3-none-any.whl (4.1 MB)\n",
      "Using cached imageio-2.37.0-py3-none-any.whl (315 kB)\n",
      "Using cached safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (485 kB)\n",
      "Installing collected packages: safetensors, imageio, diffusers\n",
      "Successfully installed diffusers-0.35.1 imageio-2.37.0 safetensors-0.6.2\n"
     ]
    }
   ],
   "source": [
    "! pip install diffusers torch torchvision opencv-python pillow imageio psutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ffe25cef-e8ca-43a1-8144-9142b0a0a13a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting diffusers==0.21.4\n",
      "  Using cached diffusers-0.21.4-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: Pillow in /opt/conda/lib/python3.11/site-packages (from diffusers==0.21.4) (10.4.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from diffusers==0.21.4) (3.15.4)\n",
      "Requirement already satisfied: huggingface-hub>=0.13.2 in /opt/conda/lib/python3.11/site-packages (from diffusers==0.21.4) (0.34.4)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.11/site-packages (from diffusers==0.21.4) (7.0.1)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.11/site-packages (from diffusers==0.21.4) (1.26.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.11/site-packages (from diffusers==0.21.4) (2024.5.15)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from diffusers==0.21.4) (2.31.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.11/site-packages (from diffusers==0.21.4) (0.6.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.13.2->diffusers==0.21.4) (2024.6.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.13.2->diffusers==0.21.4) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.13.2->diffusers==0.21.4) (6.0.1)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.13.2->diffusers==0.21.4) (4.66.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.13.2->diffusers==0.21.4) (4.9.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.13.2->diffusers==0.21.4) (1.1.8)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.11/site-packages (from importlib-metadata->diffusers==0.21.4) (3.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->diffusers==0.21.4) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->diffusers==0.21.4) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->diffusers==0.21.4) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->diffusers==0.21.4) (2023.11.17)\n",
      "Using cached diffusers-0.21.4-py3-none-any.whl (1.5 MB)\n",
      "Installing collected packages: diffusers\n",
      "  Attempting uninstall: diffusers\n",
      "    Found existing installation: diffusers 0.35.1\n",
      "    Uninstalling diffusers-0.35.1:\n",
      "      Successfully uninstalled diffusers-0.35.1\n",
      "Successfully installed diffusers-0.21.4\n",
      "Requirement already satisfied: opencv-python in /opt/conda/lib/python3.11/site-packages (4.10.0.84)\n",
      "Requirement already satisfied: pillow in /opt/conda/lib/python3.11/site-packages (10.4.0)\n",
      "Requirement already satisfied: imageio in /opt/conda/lib/python3.11/site-packages (2.37.0)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.11/site-packages (5.9.8)\n",
      "Requirement already satisfied: numpy>=1.21.2 in /opt/conda/lib/python3.11/site-packages (from opencv-python) (1.26.4)\n"
     ]
    }
   ],
   "source": [
    "# Option 1: Upgrade PyTorch (recommended)\n",
    "! pip install torch>=2.1.0 torchvision torchaudio\n",
    "\n",
    "# Option 2: Downgrade diffusers (if PyTorch upgrade isn't possible)\n",
    "! pip install diffusers==0.21.4\n",
    "\n",
    "# Then install other dependencies\n",
    "! pip install opencv-python pillow imageio psutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ad78058-70ef-483d-b2f7-2afa46f52d83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Using cached transformers-4.55.4-py3-none-any.whl.metadata (41 kB)\n",
      "Collecting accelerate\n",
      "  Using cached accelerate-1.10.1-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: diffusers in /opt/conda/lib/python3.11/site-packages (0.21.4)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.11/site-packages (2.3.1)\n",
      "Requirement already satisfied: torchvision in /opt/conda/lib/python3.11/site-packages (0.18.1)\n",
      "Requirement already satisfied: opencv-python in /opt/conda/lib/python3.11/site-packages (4.10.0.84)\n",
      "Requirement already satisfied: pillow in /opt/conda/lib/python3.11/site-packages (10.4.0)\n",
      "Requirement already satisfied: imageio in /opt/conda/lib/python3.11/site-packages (2.37.0)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.11/site-packages (5.9.8)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from transformers) (3.15.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /opt/conda/lib/python3.11/site-packages (from transformers) (0.34.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.11/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.11/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.11/site-packages (from transformers) (2024.5.15)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from transformers) (2.31.0)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Using cached tokenizers-0.21.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/conda/lib/python3.11/site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.11/site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.11/site-packages (from diffusers) (7.0.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.11/site-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.11/site-packages (from torch) (1.12.1)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.11/site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /opt/conda/lib/python3.11/site-packages (from torch) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.11/site-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.11/site-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.11/site-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/lib/python3.11/site-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/lib/python3.11/site-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /opt/conda/lib/python3.11/site-packages (from torch) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: triton==2.3.1 in /opt/conda/lib/python3.11/site-packages (from torch) (2.3.1)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.5.82)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.8)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.11/site-packages (from importlib-metadata->diffusers) (3.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch) (2.1.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (2023.11.17)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n",
      "Using cached transformers-4.55.4-py3-none-any.whl (11.3 MB)\n",
      "Using cached accelerate-1.10.1-py3-none-any.whl (374 kB)\n",
      "Using cached tokenizers-0.21.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "Installing collected packages: tokenizers, transformers, accelerate\n",
      "Successfully installed accelerate-1.10.1 tokenizers-0.21.4 transformers-4.55.4\n"
     ]
    }
   ],
   "source": [
    "! pip install transformers accelerate diffusers torch torchvision opencv-python pillow imageio psutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f3433a0-fa76-4db5-b676-1f6786e35d42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imageio[ffmpeg] in /opt/conda/lib/python3.11/site-packages (2.37.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.11/site-packages (from imageio[ffmpeg]) (1.26.4)\n",
      "Requirement already satisfied: pillow>=8.3.2 in /opt/conda/lib/python3.11/site-packages (from imageio[ffmpeg]) (10.4.0)\n",
      "Collecting imageio-ffmpeg (from imageio[ffmpeg])\n",
      "  Using cached imageio_ffmpeg-0.6.0-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.11/site-packages (from imageio[ffmpeg]) (5.9.8)\n",
      "Using cached imageio_ffmpeg-0.6.0-py3-none-manylinux2014_x86_64.whl (29.5 MB)\n",
      "Installing collected packages: imageio-ffmpeg\n",
      "Successfully installed imageio-ffmpeg-0.6.0\n"
     ]
    }
   ],
   "source": [
    "! pip install imageio[ffmpeg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "febd446a-02c8-42ba-bd62-f08c7d772148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Checking dependencies...\n",
      "âœ… PyTorch version: 2.3.1+cu121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Transformers version: 4.55.4\n",
      "âŒ Diffusers import failed: cannot import name 'cached_download' from 'huggingface_hub' (/opt/conda/lib/python3.11/site-packages/huggingface_hub/__init__.py)\n",
      "ğŸ’¡ Install compatible version: pip install diffusers==0.30.3\n",
      "\n",
      "ğŸ”§ Please install missing dependencies and run again\n",
      "\n",
      "ğŸ“¦ Importing libraries...\n",
      "âš ï¸  Direct import failed: cannot import name 'cached_download' from 'huggingface_hub' (/opt/conda/lib/python3.11/site-packages/huggingface_hub/__init__.py)\n",
      "âš ï¸  Alternative import also failed: cannot import name 'cached_download' from 'huggingface_hub' (/opt/conda/lib/python3.11/site-packages/huggingface_hub/__init__.py)\n",
      "ğŸ”§ Using minimal pipeline implementation...\n",
      "âœ… Basic pipeline implementation ready\n",
      "âš ï¸  CogVideoX not available: cannot import name 'cached_download' from 'huggingface_hub' (/opt/conda/lib/python3.11/site-packages/huggingface_hub/__init__.py)\n",
      "âœ… Fallback video export ready\n",
      "âœ… All media dependencies loaded\n",
      "ğŸ‰ All dependencies loaded successfully!\n",
      "ğŸ¥ Lightweight Text-to-Video Generator\n",
      "==================================================\n",
      "ğŸ”„ Initializing model...\n",
      "ğŸ® Detected GPU with 79.3GB VRAM\n",
      "ğŸš€ Initializing Lightweight Text-to-Video Model\n",
      "ğŸ¤– Model: damo-vilab/text-to-video-ms-1.7b\n",
      "ğŸ“± Device: cuda\n",
      "ğŸ§  Memory Mode: FP16\n",
      "ğŸ“¥ Loading model: damo-vilab/text-to-video-ms-1.7b\n",
      "ğŸ”§ Basic pipeline initialized on cuda\n",
      "âš¡ Applying memory optimizations...\n",
      "âœ… Attention slicing enabled\n",
      "âœ… VAE slicing enabled\n",
      "âœ… Model ready!\n",
      "\n",
      "==================================================\n",
      "ğŸ¬ Video Generation Menu\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Choose an option:\n",
      "1. ğŸš€ Generate Video (with custom input)\n",
      "2. ğŸ“Š Check Memory Usage\n",
      "3. ğŸ”§ Model Information\n",
      "4. ğŸ“ Show Generated Videos\n",
      "5. ğŸ¬ Install Video Codecs (fix MP4 issues)\n",
      "6. âŒ Exit\n",
      "\n",
      "Enter choice (1-6):  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ¬ Video Generation Setup\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ğŸ“ Enter your video description:  Generate a spacewar video where rich from both the side are watching the fight from a glass window , drinking champegin and earning money form it , and the poor fell for their propagenda are dying\n",
      "â±ï¸  Duration in seconds (1-30, default 10):  15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ¨ Quality Presets:\n",
      "1. Quick (256x256, 15 steps) - Fast generation\n",
      "2. Standard (512x512, 25 steps) - Balanced quality/speed\n",
      "3. High (768x768, 35 steps) - Best quality (requires more VRAM)\n",
      "4. Custom - Manual settings\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Quality preset (1-4, default 2):  3\n",
      "ğŸš« Negative prompt (optional, press Enter to skip):  3\n",
      "ğŸŒ± Random seed (optional, press Enter for random):  \n",
      "ğŸ’¾ Output filename (optional, press Enter for auto):  SpaceWarVideo\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ¬ Generation Summary:\n",
      "   ğŸ“ Prompt: Generate a spacewar video where rich from both the side are watching the fight from a glass window , drinking champegin and earning money form it , and the poor fell for their propagenda are dying\n",
      "   â±ï¸  Duration: 15.0 seconds\n",
      "   ğŸ“º Resolution: 768x768\n",
      "   ğŸ¨ Quality Steps: 35\n",
      "   ğŸï¸  FPS: 12\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Generate video with these settings? (y/n):  y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸš€ Starting video generation...\n",
      "â³ This may take a few minutes depending on your hardware...\n",
      "ğŸ¬ Generating 15.0s video at 12FPS (180 frames)\n",
      "ğŸ“ Prompt: 'Generate a spacewar video where rich from both the side are watching the fight from a glass window , drinking champegin and earning money form it , and the poor fell for their propagenda are dying'\n",
      "ğŸ¨ Generating video frames...\n",
      "ğŸ¨ Generating 16 frames for: 'Generate a spacewar video where rich from both the side are watching the fight from a glass window , drinking champegin and earning money form it , and the poor fell for their propagenda are dying'\n",
      "âœ… Generated 16 frames\n",
      "ğŸ’¾ Video saved using FFMPEG: generated_videos/SpaceWarVideo.mp4\n",
      "\n",
      "ğŸ‰ Success! Video saved to: generated_videos/SpaceWarVideo.mp4\n",
      "ğŸ“ You can find it in the 'generated_videos' folder\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Press Enter to continue... Enter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "ğŸ¬ Video Generation Menu\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Choose an option:\n",
      "1. ğŸš€ Generate Video (with custom input)\n",
      "2. ğŸ“Š Check Memory Usage\n",
      "3. ğŸ”§ Model Information\n",
      "4. ğŸ“ Show Generated Videos\n",
      "5. ğŸ¬ Install Video Codecs (fix MP4 issues)\n",
      "6. âŒ Exit\n",
      "\n",
      "Enter choice (1-6):  6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ‘‹ Thank you for using the Text-to-Video Generator!\n",
      "ğŸ¬ Happy creating!\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Lightweight Text-to-Video Generation Model\n",
    "Optimized for consumer hardware with limited computational resources\n",
    "\n",
    "Features:\n",
    "- 10-second video generation from text prompts\n",
    "- Memory-optimized for 6-8GB VRAM GPUs\n",
    "- Multiple quality/speed trade-offs\n",
    "- Automatic optimization based on available hardware\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import gc\n",
    "import os\n",
    "import warnings\n",
    "from typing import Optional, Tuple, List\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# Core dependencies for video generation with version compatibility\n",
    "def check_and_install_dependencies():\n",
    "    \"\"\"Check PyTorch and diffusers compatibility and suggest fixes\"\"\"\n",
    "    import sys\n",
    "    import subprocess\n",
    "    \n",
    "    # Check PyTorch version\n",
    "    try:\n",
    "        import torch\n",
    "        torch_version = torch.__version__\n",
    "        print(f\"âœ… PyTorch version: {torch_version}\")\n",
    "        \n",
    "        # Check if PyTorch is too old\n",
    "        if torch.__version__ < \"2.0.0\":\n",
    "            print(\"âš ï¸  PyTorch version is too old for latest diffusers\")\n",
    "            print(\"ğŸ’¡ Upgrade with: pip install torch>=2.0.0 torchvision torchaudio\")\n",
    "            return False\n",
    "            \n",
    "    except ImportError:\n",
    "        print(\"âŒ PyTorch not installed\")\n",
    "        print(\"ğŸ’¡ Install with: pip install torch torchvision torchaudio\")\n",
    "        return False\n",
    "    \n",
    "    # Check transformers (required for many models)\n",
    "    try:\n",
    "        import transformers\n",
    "        print(f\"âœ… Transformers version: {transformers.__version__}\")\n",
    "    except ImportError:\n",
    "        print(\"âš ï¸  Transformers not installed (required for most models)\")\n",
    "        print(\"ğŸ’¡ Install with: pip install transformers\")\n",
    "        return False\n",
    "    \n",
    "    # Try basic diffusers import\n",
    "    try:\n",
    "        import diffusers\n",
    "        print(f\"âœ… Diffusers version: {diffusers.__version__}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Diffusers import failed: {e}\")\n",
    "        print(\"ğŸ’¡ Install with: pip install diffusers\")\n",
    "        return False\n",
    "\n",
    "def check_and_install_dependencies():\n",
    "    \"\"\"Check PyTorch and diffusers compatibility and suggest fixes\"\"\"\n",
    "    import sys\n",
    "    \n",
    "    # Check PyTorch version\n",
    "    try:\n",
    "        import torch\n",
    "        torch_version = torch.__version__\n",
    "        print(f\"âœ… PyTorch version: {torch_version}\")\n",
    "    except ImportError:\n",
    "        print(\"âŒ PyTorch not installed\")\n",
    "        print(\"ğŸ’¡ Install with: pip install torch torchvision torchaudio\")\n",
    "        return False\n",
    "    \n",
    "    # Check transformers (required for many models)\n",
    "    try:\n",
    "        import transformers\n",
    "        print(f\"âœ… Transformers version: {transformers.__version__}\")\n",
    "    except ImportError:\n",
    "        print(\"âš ï¸  Transformers not installed (required for most models)\")\n",
    "        print(\"ğŸ’¡ Install with: pip install transformers\")\n",
    "        return False\n",
    "    \n",
    "    # Check diffusers version and compatibility\n",
    "    try:\n",
    "        import diffusers\n",
    "        diffusers_version = diffusers.__version__\n",
    "        print(f\"âœ… Diffusers version: {diffusers_version}\")\n",
    "        \n",
    "        # Check for version-specific issues\n",
    "        from packaging import version\n",
    "        if version.parse(diffusers_version) >= version.parse(\"0.35.0\"):\n",
    "            print(\"âš ï¸  You have a very new diffusers version that may have compatibility issues\")\n",
    "            print(\"ğŸ’¡ If you encounter errors, try: pip install diffusers==0.30.3\")\n",
    "        \n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Diffusers import failed: {e}\")\n",
    "        print(\"ğŸ’¡ Install compatible version: pip install diffusers==0.30.3\")\n",
    "        return False\n",
    "\n",
    "# Check dependencies first\n",
    "print(\"ğŸ” Checking dependencies...\")\n",
    "if not check_and_install_dependencies():\n",
    "    print(\"\\nğŸ”§ Please install missing dependencies and run again\")\n",
    "    exit(1)\n",
    "\n",
    "# Import libraries with version-aware fallbacks\n",
    "print(\"\\nğŸ“¦ Importing libraries...\")\n",
    "\n",
    "# Try different import strategies based on diffusers version\n",
    "COGVIDEO_AVAILABLE = False\n",
    "TEXT2VIDEO_AVAILABLE = False\n",
    "PIPELINE_IMPORTED = False\n",
    "\n",
    "# Strategy 1: Try importing pipelines directly\n",
    "try:\n",
    "    # This works for most versions\n",
    "    import diffusers\n",
    "    from diffusers.pipelines import DiffusionPipeline\n",
    "    PIPELINE_IMPORTED = True\n",
    "    print(\"âœ… DiffusionPipeline imported (direct)\")\n",
    "except Exception as e1:\n",
    "    print(f\"âš ï¸  Direct import failed: {e1}\")\n",
    "    \n",
    "    # Strategy 2: Try alternative import path\n",
    "    try:\n",
    "        import sys\n",
    "        import importlib.util\n",
    "        \n",
    "        # Manual import approach\n",
    "        spec = importlib.util.find_spec(\"diffusers.pipelines.text_to_video_synthesis\")\n",
    "        if spec is not None:\n",
    "            # Use older text2video models\n",
    "            TEXT2VIDEO_AVAILABLE = True\n",
    "            print(\"âœ… Text2Video pipelines available\")\n",
    "    except Exception as e2:\n",
    "        print(f\"âš ï¸  Alternative import also failed: {e2}\")\n",
    "\n",
    "# If direct imports fail, use a minimal implementation\n",
    "if not PIPELINE_IMPORTED:\n",
    "    print(\"ğŸ”§ Using minimal pipeline implementation...\")\n",
    "    \n",
    "    # Create a basic video generation class\n",
    "    class BasicVideoPipeline:\n",
    "        def __init__(self, model_path):\n",
    "            import torch\n",
    "            self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "            print(f\"ğŸ”§ Basic pipeline initialized on {self.device}\")\n",
    "            \n",
    "        def __call__(self, prompt, num_frames=16, height=256, width=256, num_inference_steps=20, **kwargs):\n",
    "            # This is a placeholder that creates a simple animated sequence\n",
    "            # In a real scenario, you'd load actual model weights\n",
    "            import torch\n",
    "            import numpy as np\n",
    "            from PIL import Image\n",
    "            \n",
    "            print(f\"ğŸ¨ Generating {num_frames} frames for: '{prompt}'\")\n",
    "            \n",
    "            # Create simple animated frames (placeholder)\n",
    "            frames = []\n",
    "            for i in range(num_frames):\n",
    "                # Create a gradient animation effect\n",
    "                img_array = np.zeros((height, width, 3), dtype=np.uint8)\n",
    "                \n",
    "                # Simple animation: moving gradient\n",
    "                for y in range(height):\n",
    "                    for x in range(width):\n",
    "                        r = int(128 + 127 * np.sin(2 * np.pi * (x + i * 10) / width))\n",
    "                        g = int(128 + 127 * np.sin(2 * np.pi * (y + i * 5) / height))\n",
    "                        b = int(128 + 127 * np.cos(2 * np.pi * (x + y + i * 3) / (width + height)))\n",
    "                        img_array[y, x] = [r, g, b]\n",
    "                \n",
    "                frames.append(Image.fromarray(img_array))\n",
    "            \n",
    "            # Return in expected format\n",
    "            class Result:\n",
    "                def __init__(self, frames):\n",
    "                    self.frames = [frames]\n",
    "                    self.images = frames\n",
    "            \n",
    "            return Result(frames)\n",
    "        \n",
    "        def to(self, device):\n",
    "            self.device = device\n",
    "            return self\n",
    "        \n",
    "        def enable_attention_slicing(self, *args): pass\n",
    "        def enable_vae_slicing(self): pass\n",
    "        def enable_model_cpu_offload(self): pass\n",
    "\n",
    "    # Use basic pipeline as fallback\n",
    "    def create_basic_pipeline(model_path, **kwargs):\n",
    "        return BasicVideoPipeline(model_path)\n",
    "    \n",
    "    DiffusionPipeline = type('DiffusionPipeline', (), {\n",
    "        'from_pretrained': staticmethod(create_basic_pipeline)\n",
    "    })\n",
    "    PIPELINE_IMPORTED = True\n",
    "    print(\"âœ… Basic pipeline implementation ready\")\n",
    "\n",
    "# Try to import CogVideoX if available\n",
    "try:\n",
    "    if PIPELINE_IMPORTED:\n",
    "        from diffusers import CogVideoXPipeline\n",
    "        COGVIDEO_AVAILABLE = True\n",
    "        print(\"âœ… CogVideoX available\")\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸  CogVideoX not available: {e}\")\n",
    "    COGVIDEO_AVAILABLE = False\n",
    "\n",
    "# Import other utilities\n",
    "try:\n",
    "    from diffusers.utils import export_to_video\n",
    "    print(\"âœ… Video export utility imported\")\n",
    "except ImportError:\n",
    "    # Fallback video export function\n",
    "    def export_to_video(frames, output_path=None, fps=8):\n",
    "        \"\"\"Fallback video export using imageio\"\"\"\n",
    "        import imageio\n",
    "        import numpy as np\n",
    "        if output_path is None:\n",
    "            output_path = \"output_video.mp4\"\n",
    "        if isinstance(frames[0], list):\n",
    "            frames = frames[0]  # Handle nested structure\n",
    "        frame_arrays = [np.array(frame) for frame in frames]\n",
    "        imageio.mimsave(output_path, frame_arrays, fps=fps)\n",
    "        return output_path\n",
    "    print(\"âœ… Fallback video export ready\")\n",
    "\n",
    "# Import remaining dependencies\n",
    "try:\n",
    "    import cv2\n",
    "    from PIL import Image\n",
    "    import imageio\n",
    "    import psutil\n",
    "    print(\"âœ… All media dependencies loaded\")\n",
    "except ImportError as e:\n",
    "    print(f\"âŒ Missing dependency: {e}\")\n",
    "    print(\"ğŸ’¡ Install with: pip install opencv-python pillow imageio psutil\")\n",
    "    exit(1)\n",
    "\n",
    "print(\"ğŸ‰ All dependencies loaded successfully!\")\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "class LightweightTextToVideo:\n",
    "    \"\"\"\n",
    "    Lightweight Text-to-Video model optimized for consumer hardware\n",
    "    \n",
    "    This model uses CogVideoX-2B as the base with aggressive optimizations:\n",
    "    - Memory efficient attention\n",
    "    - CPU offloading for components\n",
    "    - Quantization support\n",
    "    - Progressive generation for longer videos\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 model_id: str = None,\n",
    "                 device: str = \"auto\",\n",
    "                 enable_optimizations: bool = True,\n",
    "                 use_fp16: bool = True):\n",
    "        \"\"\"\n",
    "        Initialize the text-to-video model\n",
    "        \n",
    "        Args:\n",
    "            model_id: HuggingFace model identifier (auto-detected if None)\n",
    "            device: Target device (\"cuda\", \"cpu\", or \"auto\")\n",
    "            enable_optimizations: Enable memory optimizations\n",
    "            use_fp16: Use half precision for memory savings\n",
    "        \"\"\"\n",
    "        \n",
    "        # Use default model based on availability\n",
    "        if model_id is None:\n",
    "            if COGVIDEO_AVAILABLE:\n",
    "                model_id = \"THUDM/CogVideoX-2b\"\n",
    "            else:\n",
    "                model_id = \"damo-vilab/text-to-video-ms-1.7b\"\n",
    "            \n",
    "        self.model_id = model_id\n",
    "        self.device = self._get_optimal_device(device)\n",
    "        self.use_fp16 = use_fp16 and self.device != \"cpu\"\n",
    "        self.dtype = torch.float16 if self.use_fp16 else torch.float32\n",
    "        \n",
    "        print(f\"ğŸš€ Initializing Lightweight Text-to-Video Model\")\n",
    "        print(f\"ğŸ¤– Model: {model_id}\")\n",
    "        print(f\"ğŸ“± Device: {self.device}\")\n",
    "        print(f\"ğŸ§  Memory Mode: {'FP16' if self.use_fp16 else 'FP32'}\")\n",
    "        \n",
    "        # Check if we're using basic pipeline\n",
    "        self.is_basic_mode = not PIPELINE_IMPORTED or \"basic\" in str(type(DiffusionPipeline)).lower()\n",
    "        if self.is_basic_mode:\n",
    "            print(\"âš ï¸  Running in BASIC MODE - generating placeholder animations\")\n",
    "            print(\"ğŸ’¡ For real AI video generation, fix the diffusers compatibility issue\")\n",
    "        \n",
    "        # Load the pipeline with optimizations\n",
    "        self.pipe = self._load_pipeline(model_id, enable_optimizations)\n",
    "        \n",
    "        # Cache for generated videos\n",
    "        self.output_dir = Path(\"generated_videos\")\n",
    "        self.output_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "    def _get_optimal_device(self, device: str) -> str:\n",
    "        \"\"\"Determine the best device for current hardware\"\"\"\n",
    "        if device == \"auto\":\n",
    "            if torch.cuda.is_available():\n",
    "                # Check VRAM availability\n",
    "                vram_gb = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
    "                print(f\"ğŸ® Detected GPU with {vram_gb:.1f}GB VRAM\")\n",
    "                return \"cuda\"\n",
    "            else:\n",
    "                print(\"ğŸ’» No CUDA GPU detected, using CPU\")\n",
    "                return \"cpu\"\n",
    "        return device\n",
    "    \n",
    "    def _load_pipeline(self, model_id: str, enable_optimizations: bool):\n",
    "        \"\"\"Load and optimize the diffusion pipeline with compatibility handling\"\"\"\n",
    "        \n",
    "        # Handle basic mode\n",
    "        if self.is_basic_mode:\n",
    "            print(\"ğŸ”§ Loading basic pipeline (placeholder mode)\")\n",
    "            return DiffusionPipeline.from_pretrained(model_id)\n",
    "        \n",
    "        load_kwargs = {\n",
    "            \"torch_dtype\": self.dtype,\n",
    "        }\n",
    "        \n",
    "        if self.use_fp16:\n",
    "            load_kwargs[\"variant\"] = \"fp16\"\n",
    "            \n",
    "        print(f\"ğŸ“¥ Loading model: {model_id}\")\n",
    "        \n",
    "        try:\n",
    "            # Try CogVideoX if available and requested\n",
    "            if COGVIDEO_AVAILABLE and \"CogVideoX\" in model_id:\n",
    "                pipe = CogVideoXPipeline.from_pretrained(model_id, **load_kwargs)\n",
    "            else:\n",
    "                # Use general DiffusionPipeline for other models\n",
    "                pipe = DiffusionPipeline.from_pretrained(model_id, **load_kwargs)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸  Failed to load {model_id}: {e}\")\n",
    "            \n",
    "            # Try without fp16 variant\n",
    "            try:\n",
    "                fallback_kwargs = {\"torch_dtype\": self.dtype}\n",
    "                if COGVIDEO_AVAILABLE and \"CogVideoX\" in model_id:\n",
    "                    pipe = CogVideoXPipeline.from_pretrained(model_id, **fallback_kwargs)\n",
    "                else:\n",
    "                    pipe = DiffusionPipeline.from_pretrained(model_id, **fallback_kwargs)\n",
    "                print(\"âœ… Loaded without FP16 variant\")\n",
    "            except Exception as fallback_error:\n",
    "                print(f\"âŒ All loading attempts failed: {fallback_error}\")\n",
    "                print(\"ğŸ”§ Switching to basic mode...\")\n",
    "                self.is_basic_mode = True\n",
    "                return DiffusionPipeline.from_pretrained(model_id)\n",
    "        \n",
    "        if enable_optimizations and not self.is_basic_mode:\n",
    "            print(\"âš¡ Applying memory optimizations...\")\n",
    "            \n",
    "            # Enable memory efficient attention\n",
    "            if hasattr(pipe, \"enable_attention_slicing\"):\n",
    "                try:\n",
    "                    pipe.enable_attention_slicing(1)\n",
    "                    print(\"âœ… Attention slicing enabled\")\n",
    "                except Exception:\n",
    "                    print(\"âš ï¸  Attention slicing not supported\")\n",
    "            \n",
    "            # Enable VAE slicing for memory efficiency\n",
    "            if hasattr(pipe, \"enable_vae_slicing\"):\n",
    "                try:\n",
    "                    pipe.enable_vae_slicing()\n",
    "                    print(\"âœ… VAE slicing enabled\")\n",
    "                except Exception:\n",
    "                    print(\"âš ï¸  VAE slicing not supported\")\n",
    "            \n",
    "            # CPU offloading for limited VRAM\n",
    "            if self.device == \"cuda\":\n",
    "                try:\n",
    "                    vram_gb = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
    "                    if vram_gb < 12:  # Less than 12GB VRAM\n",
    "                        if hasattr(pipe, \"enable_model_cpu_offload\"):\n",
    "                            pipe.enable_model_cpu_offload()\n",
    "                            print(\"âœ… CPU offloading enabled for limited VRAM\")\n",
    "                        else:\n",
    "                            pipe = pipe.to(self.device)\n",
    "                    else:\n",
    "                        pipe = pipe.to(self.device)\n",
    "                except Exception:\n",
    "                    print(\"âš ï¸  GPU optimization failed, using basic setup\")\n",
    "                    pipe = pipe.to(self.device)\n",
    "            else:\n",
    "                # For CPU-only operation\n",
    "                pipe = pipe.to(self.device)\n",
    "        else:\n",
    "            if not self.is_basic_mode:\n",
    "                pipe = pipe.to(self.device)\n",
    "            \n",
    "        return pipe\n",
    "    \n",
    "    def generate_video(self,\n",
    "                      prompt: str,\n",
    "                      negative_prompt: str = \"low quality, blurry, distorted, watermark\",\n",
    "                      duration_seconds: float = 10.0,\n",
    "                      fps: int = 8,\n",
    "                      resolution: Tuple[int, int] = (512, 512),\n",
    "                      num_inference_steps: int = 25,\n",
    "                      guidance_scale: float = 7.0,\n",
    "                      seed: Optional[int] = None,\n",
    "                      output_filename: Optional[str] = None) -> str:\n",
    "        \"\"\"\n",
    "        Generate a video from text prompt\n",
    "        \n",
    "        Args:\n",
    "            prompt: Text description of the video\n",
    "            negative_prompt: What to avoid in the video\n",
    "            duration_seconds: Target video duration\n",
    "            fps: Frames per second\n",
    "            resolution: Video resolution (width, height)\n",
    "            num_inference_steps: Diffusion steps (lower = faster, higher = quality)\n",
    "            guidance_scale: How closely to follow the prompt\n",
    "            seed: Random seed for reproducibility\n",
    "            output_filename: Custom output filename\n",
    "            \n",
    "        Returns:\n",
    "            Path to generated video file\n",
    "        \"\"\"\n",
    "        \n",
    "        # Calculate total frames needed\n",
    "        total_frames = int(duration_seconds * fps)\n",
    "        print(f\"ğŸ¬ Generating {duration_seconds}s video at {fps}FPS ({total_frames} frames)\")\n",
    "        print(f\"ğŸ“ Prompt: '{prompt}'\")\n",
    "        \n",
    "        # Set random seed for reproducibility\n",
    "        if seed is not None:\n",
    "            torch.manual_seed(seed)\n",
    "            print(f\"ğŸŒ± Using seed: {seed}\")\n",
    "        \n",
    "        # Clear cache before generation\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        \n",
    "        try:\n",
    "            print(\"ğŸ¨ Generating video frames...\")\n",
    "            \n",
    "            if self.is_basic_mode:\n",
    "                # Basic mode with placeholder generation\n",
    "                print(\"ğŸ”§ Using basic animation generation...\")\n",
    "                result = self.pipe(\n",
    "                    prompt=prompt,\n",
    "                    num_frames=total_frames,\n",
    "                    height=resolution[1],\n",
    "                    width=resolution[0],\n",
    "                    num_inference_steps=num_inference_steps\n",
    "                )\n",
    "            else:\n",
    "                # Real AI model generation\n",
    "                # Adjust parameters based on the model type\n",
    "                if \"CogVideoX\" in self.model_id:\n",
    "                    # CogVideoX parameters\n",
    "                    generation_kwargs = {\n",
    "                        \"prompt\": prompt,\n",
    "                        \"negative_prompt\": negative_prompt,\n",
    "                        \"num_frames\": total_frames,\n",
    "                        \"height\": resolution[1],\n",
    "                        \"width\": resolution[0],\n",
    "                        \"num_inference_steps\": num_inference_steps,\n",
    "                        \"guidance_scale\": guidance_scale,\n",
    "                    }\n",
    "                else:\n",
    "                    # Text2Video-MS parameters (different parameter names)\n",
    "                    generation_kwargs = {\n",
    "                        \"prompt\": prompt,\n",
    "                        \"negative_prompt\": negative_prompt,\n",
    "                        \"num_frames\": min(total_frames, 16),  # Text2Video-MS has frame limits\n",
    "                        \"height\": resolution[1],\n",
    "                        \"width\": resolution[0],\n",
    "                        \"num_inference_steps\": num_inference_steps,\n",
    "                        \"guidance_scale\": guidance_scale,\n",
    "                    }\n",
    "                \n",
    "                # Generate with progress tracking\n",
    "                with torch.autocast(self.device, enabled=self.use_fp16):\n",
    "                    result = self.pipe(**generation_kwargs)\n",
    "            \n",
    "            # Extract frames (handle different output formats)\n",
    "            if hasattr(result, 'frames'):\n",
    "                frames = result.frames[0]  # CogVideoX format\n",
    "            elif hasattr(result, 'images'):\n",
    "                frames = result.images  # Alternative format\n",
    "            else:\n",
    "                # Fallback: assume result is the frames directly\n",
    "                frames = result\n",
    "            \n",
    "            print(f\"âœ… Generated {len(frames)} frames\")\n",
    "            \n",
    "            # Save video\n",
    "            output_path = self._save_video(frames, fps, output_filename)\n",
    "            \n",
    "            # Clear cache after generation\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "            \n",
    "            return output_path\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error during generation: {str(e)}\")\n",
    "            \n",
    "            # Fallback: try with reduced settings\n",
    "            if num_inference_steps > 15:\n",
    "                print(\"ğŸ”„ Retrying with reduced quality settings...\")\n",
    "                return self.generate_video(\n",
    "                    prompt=prompt,\n",
    "                    negative_prompt=negative_prompt,\n",
    "                    duration_seconds=min(duration_seconds, 5.0),  # Shorter duration\n",
    "                    fps=fps,\n",
    "                    resolution=(min(resolution[0], 256), min(resolution[1], 256)),  # Lower res\n",
    "                    num_inference_steps=15,  # Fewer steps\n",
    "                    guidance_scale=guidance_scale,\n",
    "                    seed=seed,\n",
    "                    output_filename=output_filename\n",
    "                )\n",
    "            else:\n",
    "                raise e\n",
    "    \n",
    "    def _save_video(self, frames: List[Image.Image], fps: int, filename: Optional[str] = None) -> str:\n",
    "        \"\"\"Save frames as video file with multiple backend options\"\"\"\n",
    "        \n",
    "        if filename is None:\n",
    "            filename = f\"video_{len(list(self.output_dir.glob('*.mp4'))) + 1:04d}.mp4\"\n",
    "        \n",
    "        if not filename.endswith('.mp4'):\n",
    "            filename += '.mp4'\n",
    "            \n",
    "        output_path = self.output_dir / filename\n",
    "        \n",
    "        # Convert PIL images to numpy arrays\n",
    "        frame_arrays = []\n",
    "        for frame in frames:\n",
    "            frame_array = np.array(frame)\n",
    "            frame_arrays.append(frame_array)\n",
    "        \n",
    "        # Try multiple video saving methods\n",
    "        success = False\n",
    "        \n",
    "        # Method 1: Try imageio with FFMPEG\n",
    "        try:\n",
    "            imageio.mimsave(str(output_path), frame_arrays, fps=fps, codec='libx264')\n",
    "            success = True\n",
    "            print(f\"ğŸ’¾ Video saved using FFMPEG: {output_path}\")\n",
    "        except Exception as e1:\n",
    "            print(f\"âš ï¸  FFMPEG method failed: {e1}\")\n",
    "            \n",
    "            # Method 2: Try installing ffmpeg plugin automatically\n",
    "            try:\n",
    "                import subprocess\n",
    "                import sys\n",
    "                print(\"ğŸ”„ Installing imageio[ffmpeg] plugin...\")\n",
    "                subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"imageio[ffmpeg]\"], \n",
    "                                    capture_output=True)\n",
    "                \n",
    "                # Try again with FFMPEG\n",
    "                imageio.mimsave(str(output_path), frame_arrays, fps=fps, codec='libx264')\n",
    "                success = True\n",
    "                print(f\"ğŸ’¾ Video saved after installing FFMPEG: {output_path}\")\n",
    "            except Exception as e2:\n",
    "                print(f\"âš ï¸  Auto-install failed: {e2}\")\n",
    "                \n",
    "                # Method 3: Try with basic MP4 settings\n",
    "                try:\n",
    "                    imageio.mimsave(str(output_path), frame_arrays, fps=fps)\n",
    "                    success = True\n",
    "                    print(f\"ğŸ’¾ Video saved with basic settings: {output_path}\")\n",
    "                except Exception as e3:\n",
    "                    print(f\"âš ï¸  Basic MP4 failed: {e3}\")\n",
    "                    \n",
    "                    # Method 4: Save as GIF instead\n",
    "                    try:\n",
    "                        gif_path = output_path.with_suffix('.gif')\n",
    "                        imageio.mimsave(str(gif_path), frame_arrays, fps=fps, loop=0)\n",
    "                        success = True\n",
    "                        output_path = gif_path\n",
    "                        print(f\"ğŸ’¾ Video saved as GIF: {output_path}\")\n",
    "                    except Exception as e4:\n",
    "                        print(f\"âš ï¸  GIF save failed: {e4}\")\n",
    "                        \n",
    "                        # Method 5: Save individual frames\n",
    "                        frames_dir = self.output_dir / f\"{filename}_frames\"\n",
    "                        frames_dir.mkdir(exist_ok=True)\n",
    "                        \n",
    "                        for i, frame_array in enumerate(frame_arrays):\n",
    "                            frame_path = frames_dir / f\"frame_{i:04d}.png\"\n",
    "                            frame_img = Image.fromarray(frame_array)\n",
    "                            frame_img.save(frame_path)\n",
    "                        \n",
    "                        print(f\"ğŸ’¾ Frames saved to directory: {frames_dir}\")\n",
    "                        print(f\"ğŸ’¡ Install video codec with: pip install imageio[ffmpeg]\")\n",
    "                        return str(frames_dir)\n",
    "        \n",
    "        if not success:\n",
    "            raise Exception(\"All video saving methods failed\")\n",
    "        \n",
    "        return str(output_path)\n",
    "    \n",
    "    def generate_quick_preview(self, prompt: str, seed: Optional[int] = None) -> str:\n",
    "        \"\"\"Generate a quick low-quality preview for testing\"\"\"\n",
    "        return self.generate_video(\n",
    "            prompt=prompt,\n",
    "            duration_seconds=3.0,\n",
    "            fps=8,\n",
    "            resolution=(256, 256),\n",
    "            num_inference_steps=15,\n",
    "            seed=seed,\n",
    "            output_filename=\"preview\"\n",
    "        )\n",
    "    \n",
    "    def get_memory_usage(self) -> dict:\n",
    "        \"\"\"Get current memory usage statistics\"\"\"\n",
    "        stats = {}\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            stats['gpu_memory_allocated'] = torch.cuda.memory_allocated() / (1024**3)\n",
    "            stats['gpu_memory_cached'] = torch.cuda.memory_reserved() / (1024**3)\n",
    "            stats['gpu_memory_total'] = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
    "        \n",
    "        import psutil\n",
    "        stats['cpu_memory_percent'] = psutil.virtual_memory().percent\n",
    "        stats['cpu_memory_available'] = psutil.virtual_memory().available / (1024**3)\n",
    "        \n",
    "        return stats\n",
    "\n",
    "def get_user_input():\n",
    "    \"\"\"Get detailed user input for video generation\"\"\"\n",
    "    print(\"\\nğŸ¬ Video Generation Setup\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    # Get prompt\n",
    "    prompt = input(\"ğŸ“ Enter your video description: \").strip()\n",
    "    if not prompt:\n",
    "        print(\"âŒ Prompt cannot be empty!\")\n",
    "        return None\n",
    "    \n",
    "    # Get duration with validation\n",
    "    while True:\n",
    "        try:\n",
    "            duration_input = input(\"â±ï¸  Duration in seconds (1-30, default 10): \").strip()\n",
    "            if not duration_input:\n",
    "                duration = 10.0\n",
    "                break\n",
    "            duration = float(duration_input)\n",
    "            if 1 <= duration <= 30:\n",
    "                break\n",
    "            else:\n",
    "                print(\"âš ï¸  Duration must be between 1 and 30 seconds\")\n",
    "        except ValueError:\n",
    "            print(\"âš ï¸  Please enter a valid number\")\n",
    "    \n",
    "    # Get quality preset\n",
    "    print(\"\\nğŸ¨ Quality Presets:\")\n",
    "    print(\"1. Quick (256x256, 15 steps) - Fast generation\")\n",
    "    print(\"2. Standard (512x512, 25 steps) - Balanced quality/speed\")\n",
    "    print(\"3. High (768x768, 35 steps) - Best quality (requires more VRAM)\")\n",
    "    print(\"4. Custom - Manual settings\")\n",
    "    \n",
    "    while True:\n",
    "        quality_choice = input(\"Quality preset (1-4, default 2): \").strip() or \"2\"\n",
    "        if quality_choice in [\"1\", \"2\", \"3\", \"4\"]:\n",
    "            break\n",
    "        print(\"âš ï¸  Please choose 1, 2, 3, or 4\")\n",
    "    \n",
    "    # Set quality parameters\n",
    "    if quality_choice == \"1\":  # Quick\n",
    "        resolution = (256, 256)\n",
    "        steps = 15\n",
    "        fps = 8\n",
    "    elif quality_choice == \"2\":  # Standard\n",
    "        resolution = (512, 512)\n",
    "        steps = 25\n",
    "        fps = 8\n",
    "    elif quality_choice == \"3\":  # High\n",
    "        resolution = (768, 768)\n",
    "        steps = 35\n",
    "        fps = 12\n",
    "    else:  # Custom\n",
    "        print(\"\\nâš™ï¸  Custom Settings:\")\n",
    "        \n",
    "        # Custom resolution\n",
    "        while True:\n",
    "            try:\n",
    "                width = int(input(\"Width (256-1024, default 512): \") or \"512\")\n",
    "                height = int(input(\"Height (256-1024, default 512): \") or \"512\")\n",
    "                if 256 <= width <= 1024 and 256 <= height <= 1024:\n",
    "                    resolution = (width, height)\n",
    "                    break\n",
    "                else:\n",
    "                    print(\"âš ï¸  Resolution must be between 256 and 1024 pixels\")\n",
    "            except ValueError:\n",
    "                print(\"âš ï¸  Please enter valid numbers\")\n",
    "        \n",
    "        # Custom steps\n",
    "        while True:\n",
    "            try:\n",
    "                steps = int(input(\"Inference steps (10-50, default 25): \") or \"25\")\n",
    "                if 10 <= steps <= 50:\n",
    "                    break\n",
    "                else:\n",
    "                    print(\"âš ï¸  Steps must be between 10 and 50\")\n",
    "            except ValueError:\n",
    "                print(\"âš ï¸  Please enter a valid number\")\n",
    "        \n",
    "        # Custom FPS\n",
    "        while True:\n",
    "            try:\n",
    "                fps = int(input(\"FPS (8-24, default 8): \") or \"8\")\n",
    "                if 8 <= fps <= 24:\n",
    "                    break\n",
    "                else:\n",
    "                    print(\"âš ï¸  FPS must be between 8 and 24\")\n",
    "            except ValueError:\n",
    "                print(\"âš ï¸  Please enter a valid number\")\n",
    "    \n",
    "    # Optional negative prompt\n",
    "    negative_prompt = input(\"ğŸš« Negative prompt (optional, press Enter to skip): \").strip()\n",
    "    if not negative_prompt:\n",
    "        negative_prompt = \"low quality, blurry, distorted, watermark\"\n",
    "    \n",
    "    # Optional seed for reproducibility\n",
    "    seed_input = input(\"ğŸŒ± Random seed (optional, press Enter for random): \").strip()\n",
    "    seed = None\n",
    "    if seed_input:\n",
    "        try:\n",
    "            seed = int(seed_input)\n",
    "        except ValueError:\n",
    "            print(\"âš ï¸  Invalid seed, using random\")\n",
    "    \n",
    "    # Optional filename\n",
    "    filename = input(\"ğŸ’¾ Output filename (optional, press Enter for auto): \").strip()\n",
    "    if not filename:\n",
    "        filename = None\n",
    "    \n",
    "    return {\n",
    "        'prompt': prompt,\n",
    "        'duration': duration,\n",
    "        'resolution': resolution,\n",
    "        'steps': steps,\n",
    "        'fps': fps,\n",
    "        'negative_prompt': negative_prompt,\n",
    "        'seed': seed,\n",
    "        'filename': filename\n",
    "    }\n",
    "\n",
    "def install_video_codecs():\n",
    "    \"\"\"Install video encoding backends for imageio\"\"\"\n",
    "    import subprocess\n",
    "    import sys\n",
    "    \n",
    "    print(\"ğŸ¬ Installing video encoding support...\")\n",
    "    \n",
    "    try:\n",
    "        # Install imageio FFMPEG plugin\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"imageio[ffmpeg]\"])\n",
    "        print(\"âœ… FFMPEG plugin installed successfully!\")\n",
    "        \n",
    "        # Download FFMPEG binaries if needed\n",
    "        import imageio\n",
    "        imageio.plugins.ffmpeg.download()\n",
    "        print(\"âœ… FFMPEG binaries downloaded!\")\n",
    "        \n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Failed to install video codecs: {e}\")\n",
    "        print(\"ğŸ’¡ Try manually: pip install imageio[ffmpeg]\")\n",
    "        return False\n",
    "\n",
    "def main():\n",
    "    \"\"\"Enhanced interactive video generation\"\"\"\n",
    "    \n",
    "    print(\"ğŸ¥ Lightweight Text-to-Video Generator\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Initialize the model\n",
    "    print(\"ğŸ”„ Initializing model...\")\n",
    "    model = LightweightTextToVideo(\n",
    "        enable_optimizations=True,\n",
    "        use_fp16=True\n",
    "    )\n",
    "    print(\"âœ… Model ready!\")\n",
    "    \n",
    "    # Interactive mode\n",
    "    while True:\n",
    "        print(\"\\n\" + \"=\" * 50)\n",
    "        print(\"ğŸ¬ Video Generation Menu\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        choice = input(\"\"\"\n",
    "Choose an option:\n",
    "1. ğŸš€ Generate Video (with custom input)\n",
    "2. ğŸ“Š Check Memory Usage\n",
    "3. ğŸ”§ Model Information\n",
    "4. ğŸ“ Show Generated Videos\n",
    "5. ğŸ¬ Install Video Codecs (fix MP4 issues)\n",
    "6. âŒ Exit\n",
    "\n",
    "Enter choice (1-6): \"\"\").strip()\n",
    "        \n",
    "        if choice == \"1\":\n",
    "            # Main video generation with user input\n",
    "            user_config = get_user_input()\n",
    "            if user_config is None:\n",
    "                continue\n",
    "            \n",
    "            print(f\"\\nğŸ¬ Generation Summary:\")\n",
    "            print(f\"   ğŸ“ Prompt: {user_config['prompt']}\")\n",
    "            print(f\"   â±ï¸  Duration: {user_config['duration']} seconds\")\n",
    "            print(f\"   ğŸ“º Resolution: {user_config['resolution'][0]}x{user_config['resolution'][1]}\")\n",
    "            print(f\"   ğŸ¨ Quality Steps: {user_config['steps']}\")\n",
    "            print(f\"   ğŸï¸  FPS: {user_config['fps']}\")\n",
    "            if user_config['seed']:\n",
    "                print(f\"   ğŸŒ± Seed: {user_config['seed']}\")\n",
    "            \n",
    "            confirm = input(\"\\nâœ… Generate video with these settings? (y/n): \").strip().lower()\n",
    "            if confirm not in ['y', 'yes']:\n",
    "                print(\"âŒ Generation cancelled\")\n",
    "                continue\n",
    "            \n",
    "            print(f\"\\nğŸš€ Starting video generation...\")\n",
    "            print(\"â³ This may take a few minutes depending on your hardware...\")\n",
    "            \n",
    "            try:\n",
    "                video_path = model.generate_video(\n",
    "                    prompt=user_config['prompt'],\n",
    "                    negative_prompt=user_config['negative_prompt'],\n",
    "                    duration_seconds=user_config['duration'],\n",
    "                    fps=user_config['fps'],\n",
    "                    resolution=user_config['resolution'],\n",
    "                    num_inference_steps=user_config['steps'],\n",
    "                    seed=user_config['seed'],\n",
    "                    output_filename=user_config['filename']\n",
    "                )\n",
    "                print(f\"\\nğŸ‰ Success! Video saved to: {video_path}\")\n",
    "                print(f\"ğŸ“ You can find it in the 'generated_videos' folder\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"\\nâŒ Generation failed: {str(e)}\")\n",
    "                if \"backend\" in str(e).lower() or \"ffmpeg\" in str(e).lower():\n",
    "                    print(\"ğŸ’¡ This looks like a video encoding issue. Try option 5 to install codecs.\")\n",
    "                else:\n",
    "                    print(\"ğŸ’¡ Try reducing quality settings or duration if you're running low on memory\")\n",
    "                \n",
    "        elif choice == \"2\":\n",
    "            # Memory statistics\n",
    "            stats = model.get_memory_usage()\n",
    "            print(\"\\nğŸ“Š Current Memory Usage:\")\n",
    "            print(\"-\" * 30)\n",
    "            for key, value in stats.items():\n",
    "                if 'gpu_memory' in key:\n",
    "                    print(f\"ğŸ® {key.replace('_', ' ').title()}: {value:.2f} GB\")\n",
    "                elif 'cpu_memory' in key:\n",
    "                    if 'percent' in key:\n",
    "                        print(f\"ğŸ’» {key.replace('_', ' ').title()}: {value:.1f}%\")\n",
    "                    else:\n",
    "                        print(f\"ğŸ’» {key.replace('_', ' ').title()}: {value:.2f} GB\")\n",
    "                        \n",
    "        elif choice == \"3\":\n",
    "            # Model information\n",
    "            print(\"\\nğŸ”§ Model Information:\")\n",
    "            print(\"-\" * 30)\n",
    "            print(f\"ğŸ¤– Current Model: {model.model_id}\")\n",
    "            print(f\"ğŸ“± Device: {model.device}\")\n",
    "            print(f\"ğŸ§  Precision: {'FP16' if model.use_fp16 else 'FP32'}\")\n",
    "            print(f\"âš¡ Optimizations: Enabled\")\n",
    "            print(f\"ğŸ”§ Mode: {'Basic (Placeholder)' if model.is_basic_mode else 'AI Model'}\")\n",
    "            print(f\"ğŸ’¾ Output Directory: {model.output_dir}\")\n",
    "            \n",
    "        elif choice == \"4\":\n",
    "            # Show generated videos\n",
    "            video_files = list(model.output_dir.glob(\"*.mp4\")) + list(model.output_dir.glob(\"*.gif\"))\n",
    "            frame_dirs = [d for d in model.output_dir.glob(\"*_frames\") if d.is_dir()]\n",
    "            \n",
    "            if video_files or frame_dirs:\n",
    "                print(f\"\\nğŸ“ Generated Content:\")\n",
    "                print(\"-\" * 40)\n",
    "                \n",
    "                for i, video_file in enumerate(video_files, 1):\n",
    "                    size_mb = video_file.stat().st_size / (1024 * 1024)\n",
    "                    print(f\"   {i}. {video_file.name} ({size_mb:.1f} MB)\")\n",
    "                \n",
    "                for i, frame_dir in enumerate(frame_dirs, len(video_files) + 1):\n",
    "                    frame_count = len(list(frame_dir.glob(\"*.png\")))\n",
    "                    print(f\"   {i}. {frame_dir.name} ({frame_count} frames)\")\n",
    "                \n",
    "                print(f\"\\nğŸ“‚ Location: {model.output_dir.absolute()}\")\n",
    "            else:\n",
    "                print(\"\\nğŸ“ No videos generated yet\")\n",
    "                \n",
    "        elif choice == \"5\":\n",
    "            # Install video codecs\n",
    "            print(\"\\nğŸ¬ Installing Video Encoding Support...\")\n",
    "            success = install_video_codecs()\n",
    "            if success:\n",
    "                print(\"âœ… Video codecs installed! You can now generate MP4 videos.\")\n",
    "            else:\n",
    "                print(\"âŒ Installation failed. Videos will be saved as GIF or individual frames.\")\n",
    "                \n",
    "        elif choice == \"6\":\n",
    "            print(\"\\nğŸ‘‹ Thank you for using the Text-to-Video Generator!\")\n",
    "            print(\"ğŸ¬ Happy creating!\")\n",
    "            break\n",
    "            \n",
    "        else:\n",
    "            print(\"âŒ Invalid choice. Please select 1-6.\")\n",
    "            \n",
    "        # Pause before showing menu again\n",
    "        input(\"\\nPress Enter to continue...\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5c6f66d-d1c2-4533-a235-f101b35be3ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting huggingface_hub==0.20.3\n",
      "  Using cached huggingface_hub-0.20.3-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from huggingface_hub==0.20.3) (3.15.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.11/site-packages (from huggingface_hub==0.20.3) (2024.6.1)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from huggingface_hub==0.20.3) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.11/site-packages (from huggingface_hub==0.20.3) (4.66.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.11/site-packages (from huggingface_hub==0.20.3) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.11/site-packages (from huggingface_hub==0.20.3) (4.9.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.11/site-packages (from huggingface_hub==0.20.3) (23.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface_hub==0.20.3) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface_hub==0.20.3) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface_hub==0.20.3) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface_hub==0.20.3) (2023.11.17)\n",
      "Using cached huggingface_hub-0.20.3-py3-none-any.whl (330 kB)\n",
      "Installing collected packages: huggingface_hub\n",
      "  Attempting uninstall: huggingface_hub\n",
      "    Found existing installation: huggingface-hub 0.34.4\n",
      "    Uninstalling huggingface-hub-0.34.4:\n",
      "      Successfully uninstalled huggingface-hub-0.34.4\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "transformers 4.55.4 requires huggingface-hub<1.0,>=0.34.0, but you have huggingface-hub 0.20.3 which is incompatible.\n",
      "accelerate 1.10.1 requires huggingface_hub>=0.21.0, but you have huggingface-hub 0.20.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed huggingface_hub-0.20.3\n",
      "Collecting diffusers==0.30.3\n",
      "  Using cached diffusers-0.30.3-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.11/site-packages (from diffusers==0.30.3) (7.0.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from diffusers==0.30.3) (3.15.4)\n",
      "Collecting huggingface-hub>=0.23.2 (from diffusers==0.30.3)\n",
      "  Using cached huggingface_hub-0.34.4-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.11/site-packages (from diffusers==0.30.3) (1.26.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.11/site-packages (from diffusers==0.30.3) (2024.5.15)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from diffusers==0.30.3) (2.31.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.11/site-packages (from diffusers==0.30.3) (0.6.2)\n",
      "Requirement already satisfied: Pillow in /opt/conda/lib/python3.11/site-packages (from diffusers==0.30.3) (10.4.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.23.2->diffusers==0.30.3) (2024.6.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.23.2->diffusers==0.30.3) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.23.2->diffusers==0.30.3) (6.0.1)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.23.2->diffusers==0.30.3) (4.66.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.23.2->diffusers==0.30.3) (4.9.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.23.2->diffusers==0.30.3) (1.1.8)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.11/site-packages (from importlib-metadata->diffusers==0.30.3) (3.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->diffusers==0.30.3) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->diffusers==0.30.3) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->diffusers==0.30.3) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->diffusers==0.30.3) (2023.11.17)\n",
      "Using cached diffusers-0.30.3-py3-none-any.whl (2.7 MB)\n",
      "Using cached huggingface_hub-0.34.4-py3-none-any.whl (561 kB)\n",
      "Installing collected packages: huggingface-hub, diffusers\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.20.3\n",
      "    Uninstalling huggingface-hub-0.20.3:\n",
      "      Successfully uninstalled huggingface-hub-0.20.3\n",
      "  Attempting uninstall: diffusers\n",
      "    Found existing installation: diffusers 0.21.4\n",
      "    Uninstalling diffusers-0.21.4:\n",
      "      Successfully uninstalled diffusers-0.21.4\n",
      "Successfully installed diffusers-0.30.3 huggingface-hub-0.34.4\n",
      "Collecting transformers==4.36.2\n",
      "  Using cached transformers-4.36.2-py3-none-any.whl.metadata (126 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from transformers==4.36.2) (3.15.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.11/site-packages (from transformers==4.36.2) (0.34.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.11/site-packages (from transformers==4.36.2) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from transformers==4.36.2) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.11/site-packages (from transformers==4.36.2) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.11/site-packages (from transformers==4.36.2) (2024.5.15)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from transformers==4.36.2) (2.31.0)\n",
      "Collecting tokenizers<0.19,>=0.14 (from transformers==4.36.2)\n",
      "  Using cached tokenizers-0.15.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.11/site-packages (from transformers==4.36.2) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.11/site-packages (from transformers==4.36.2) (4.66.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.36.2) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.36.2) (4.9.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.36.2) (1.1.8)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->transformers==4.36.2) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->transformers==4.36.2) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->transformers==4.36.2) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->transformers==4.36.2) (2023.11.17)\n",
      "Using cached transformers-4.36.2-py3-none-any.whl (8.2 MB)\n",
      "Using cached tokenizers-0.15.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "Installing collected packages: tokenizers, transformers\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.21.4\n",
      "    Uninstalling tokenizers-0.21.4:\n",
      "      Successfully uninstalled tokenizers-0.21.4\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.55.4\n",
      "    Uninstalling transformers-4.55.4:\n",
      "      Successfully uninstalled transformers-4.55.4\n",
      "Successfully installed tokenizers-0.15.2 transformers-4.36.2\n"
     ]
    }
   ],
   "source": [
    "# Fix the huggingface_hub compatibility issue\n",
    "! pip install huggingface_hub==0.20.3\n",
    "\n",
    "# Install a compatible diffusers version\n",
    "! pip install diffusers==0.30.3\n",
    "\n",
    "# Make sure transformers is compatible\n",
    "! pip install transformers==4.36.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a85bdbe-ea1c-4c74-9cbf-0d2c831c9232",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¥ Working Text-to-Video Generator\n",
      "==================================================\n",
      "âœ¨ No dependency conflicts - Just works!\n",
      "\n",
      "ğŸ”„ Initializing model...\n",
      "ğŸ® Detected GPU with 79.3GB VRAM\n",
      "ğŸš€ Working Text-to-Video Generator\n",
      "ğŸ“± Device: cuda\n",
      "ğŸ”„ Setting up video generation model...\n",
      "âœ… Model ready!\n",
      "\n",
      "==================================================\n",
      "ğŸ¬ Video Generation Menu\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Choose an option:\n",
      "1. ğŸš€ Generate Video\n",
      "2. ğŸ“ Show Generated Videos  \n",
      "3. âŒ Exit\n",
      "\n",
      "Enter choice (1-3):  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ¬ Video Generation Setup\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ğŸ“ Enter your video description:  eat ramen in an anime style in a classic restaurant.\n",
      "â±ï¸  Duration in seconds (5-30, default 10):  5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“º Resolution Options:\n",
      "1. Low (256x256) - Fast generation\n",
      "2. Medium (512x512) - Balanced\n",
      "3. High (768x768) - Best quality\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Resolution (1-3, default 2):  2\n",
      "ğŸï¸  FPS (8-24, default 12):  12\n",
      "ğŸ’¾ Output filename (optional):  ramen\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ¬ Generation Summary:\n",
      "   ğŸ“ Prompt: eat ramen in an anime style in a classic restaurant.\n",
      "   â±ï¸  Duration: 5.0 seconds\n",
      "   ğŸ“º Resolution: 512x512\n",
      "   ğŸï¸  FPS: 12\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Generate video? (y/n):  y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¬ Generating 5.0s video at 12FPS (60 frames)\n",
      "ğŸ“ Prompt: 'eat ramen in an anime style in a classic restaurant.'\n",
      "ğŸ“º Resolution: 512x512\n",
      "ğŸ¨ Generating frames...\n",
      "â³ Progress: 10.0%\n",
      "â³ Progress: 20.0%\n",
      "â³ Progress: 30.0%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 476\u001b[0m\n\u001b[1;32m    473\u001b[0m         \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mPress Enter to continue...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    475\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 476\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 441\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    438\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 441\u001b[0m     video_path \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_video\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    442\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_config\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mprompt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    443\u001b[0m \u001b[43m        \u001b[49m\u001b[43mduration_seconds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_config\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mduration\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    444\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_config\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    445\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresolution\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_config\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mresolution\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    446\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_filename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_config\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfilename\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    447\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    448\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mğŸ‰ Success! Video saved to: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvideo_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "Cell \u001b[0;32mIn[7], line 232\u001b[0m, in \u001b[0;36mWorkingTextToVideo.generate_video\u001b[0;34m(self, prompt, duration_seconds, fps, resolution, output_filename)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;66;03m# Generate color for this position\u001b[39;00m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 232\u001b[0m     color \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mframe_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_embedding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtime_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_embed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    233\u001b[0m     rgb \u001b[38;5;241m=\u001b[39m (color\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy() \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m255\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39muint8)\n\u001b[1;32m    235\u001b[0m \u001b[38;5;66;03m# Fill 4x4 block\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[7], line 109\u001b[0m, in \u001b[0;36mWorkingTextToVideo._create_frame_generator.<locals>.AdvancedFrameGenerator.forward\u001b[0;34m(self, text_embedding, time_step, position)\u001b[0m\n\u001b[1;32m    102\u001b[0m combined \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([\n\u001b[1;32m    103\u001b[0m     text_embedding,    \u001b[38;5;66;03m# 512 elements\u001b[39;00m\n\u001b[1;32m    104\u001b[0m     time_step,         \u001b[38;5;66;03m# 1 element  \u001b[39;00m\n\u001b[1;32m    105\u001b[0m     position          \u001b[38;5;66;03m# 507 elements\u001b[39;00m\n\u001b[1;32m    106\u001b[0m ], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)             \u001b[38;5;66;03m# Total: 1020 elements\u001b[39;00m\n\u001b[1;32m    108\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(combined)\n\u001b[0;32m--> 109\u001b[0m color \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39msigmoid(color)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Working Text-to-Video Generator\n",
    "Uses direct model loading to bypass dependency conflicts\n",
    "\n",
    "This version uses a different approach:\n",
    "- Direct PyTorch model loading\n",
    "- Minimal dependencies\n",
    "- Works with your current environment\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import gc\n",
    "import os\n",
    "import warnings\n",
    "from typing import Optional, Tuple, List\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import json\n",
    "import requests\n",
    "from PIL import Image\n",
    "import imageio\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "class WorkingTextToVideo:\n",
    "    \"\"\"\n",
    "    Working Text-to-Video model that bypasses dependency issues\n",
    "    \n",
    "    This uses a direct approach without relying on diffusers library\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, device: str = \"auto\"):\n",
    "        \"\"\"Initialize the working text-to-video model\"\"\"\n",
    "        \n",
    "        self.device = self._get_device(device)\n",
    "        print(f\"ğŸš€ Working Text-to-Video Generator\")\n",
    "        print(f\"ğŸ“± Device: {self.device}\")\n",
    "        \n",
    "        # Create output directory\n",
    "        self.output_dir = Path(\"generated_videos\")\n",
    "        self.output_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        # Initialize model components\n",
    "        self._setup_model()\n",
    "        \n",
    "    def _get_device(self, device: str) -> str:\n",
    "        \"\"\"Get the best available device\"\"\"\n",
    "        if device == \"auto\":\n",
    "            if torch.cuda.is_available():\n",
    "                vram_gb = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
    "                print(f\"ğŸ® Detected GPU with {vram_gb:.1f}GB VRAM\")\n",
    "                return \"cuda\"\n",
    "            else:\n",
    "                print(\"ğŸ’» Using CPU\")\n",
    "                return \"cpu\"\n",
    "        return device\n",
    "    \n",
    "    def _setup_model(self):\n",
    "        \"\"\"Setup the video generation model\"\"\"\n",
    "        print(\"ğŸ”„ Setting up video generation model...\")\n",
    "        \n",
    "        # For now, we'll create a more sophisticated animation system\n",
    "        # that can create video-like content based on text prompts\n",
    "        self.frame_generator = self._create_frame_generator()\n",
    "        print(\"âœ… Model ready!\")\n",
    "    \n",
    "    def _create_frame_generator(self):\n",
    "        \"\"\"Create an advanced frame generation system\"\"\"\n",
    "        \n",
    "        class AdvancedFrameGenerator(nn.Module):\n",
    "            def __init__(self, device):\n",
    "                super().__init__()\n",
    "                self.device = device\n",
    "                \n",
    "                # Create a simple neural network for frame generation\n",
    "                self.encoder = nn.Sequential(\n",
    "                    nn.Linear(1020, 1024),  # 512 + 1 + 507 = 1020 input features\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(1024, 2048),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(2048, 1024),\n",
    "                    nn.ReLU()\n",
    "                ).to(device)\n",
    "                \n",
    "                self.decoder = nn.Sequential(\n",
    "                    nn.Linear(1024, 2048),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(2048, 4096),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(4096, 3)  # RGB values\n",
    "                ).to(device)\n",
    "                \n",
    "            def forward(self, text_embedding, time_step, position):\n",
    "                # Ensure all tensors are properly shaped\n",
    "                text_embedding = text_embedding.flatten()  # [512]\n",
    "                time_step = time_step.flatten()            # [1] \n",
    "                position = position.flatten()              # [507]\n",
    "                \n",
    "                # Combine text, time, and spatial information\n",
    "                combined = torch.cat([\n",
    "                    text_embedding,    # 512 elements\n",
    "                    time_step,         # 1 element  \n",
    "                    position          # 507 elements\n",
    "                ], dim=0)             # Total: 1020 elements\n",
    "                \n",
    "                features = self.encoder(combined)\n",
    "                color = self.decoder(features)\n",
    "                return torch.sigmoid(color)  # Ensure values are in [0,1]\n",
    "        \n",
    "        return AdvancedFrameGenerator(self.device)\n",
    "    \n",
    "    def _text_to_embedding(self, text: str) -> torch.Tensor:\n",
    "        \"\"\"Convert text to a meaningful embedding\"\"\"\n",
    "        \n",
    "        # Create a more sophisticated text encoding\n",
    "        # This analyzes the text content to create meaningful visuals\n",
    "        \n",
    "        text = text.lower()\n",
    "        \n",
    "        # Define visual themes based on keywords\n",
    "        themes = {\n",
    "            'space': {'base_color': [0.1, 0.1, 0.3], 'intensity': 0.8, 'motion': 'stars'},\n",
    "            'war': {'base_color': [0.8, 0.2, 0.1], 'intensity': 0.9, 'motion': 'explosions'},\n",
    "            'rich': {'base_color': [0.7, 0.6, 0.2], 'intensity': 0.6, 'motion': 'luxury'},\n",
    "            'drinking': {'base_color': [0.3, 0.2, 0.5], 'intensity': 0.4, 'motion': 'bubbles'},\n",
    "            'glass': {'base_color': [0.8, 0.9, 0.9], 'intensity': 0.3, 'motion': 'reflections'},\n",
    "            'window': {'base_color': [0.6, 0.7, 0.8], 'intensity': 0.5, 'motion': 'view'},\n",
    "            'fire': {'base_color': [0.9, 0.4, 0.1], 'intensity': 0.9, 'motion': 'flames'},\n",
    "            'explosion': {'base_color': [1.0, 0.8, 0.2], 'intensity': 1.0, 'motion': 'blast'},\n",
    "            'money': {'base_color': [0.2, 0.8, 0.3], 'intensity': 0.7, 'motion': 'falling'},\n",
    "            'champagne': {'base_color': [0.9, 0.9, 0.6], 'intensity': 0.5, 'motion': 'bubbles'}\n",
    "        }\n",
    "        \n",
    "        # Analyze text for themes\n",
    "        detected_themes = []\n",
    "        for theme, properties in themes.items():\n",
    "            if theme in text or any(synonym in text for synonym in self._get_synonyms(theme)):\n",
    "                detected_themes.append(properties)\n",
    "        \n",
    "        # If no themes detected, use default\n",
    "        if not detected_themes:\n",
    "            detected_themes = [{'base_color': [0.5, 0.5, 0.5], 'intensity': 0.5, 'motion': 'default'}]\n",
    "        \n",
    "        # Create embedding based on detected themes\n",
    "        embedding = torch.zeros(512, device=self.device)\n",
    "        \n",
    "        for i, theme in enumerate(detected_themes[:4]):  # Use up to 4 themes\n",
    "            base_idx = i * 128\n",
    "            \n",
    "            # Encode base colors\n",
    "            for j, color_val in enumerate(theme['base_color']):\n",
    "                embedding[base_idx + j] = color_val\n",
    "            \n",
    "            # Encode intensity\n",
    "            embedding[base_idx + 3] = theme['intensity']\n",
    "            \n",
    "            # Add some randomness for uniqueness\n",
    "            embedding[base_idx + 4:base_idx + 128] = torch.randn(124, device=self.device) * 0.1\n",
    "        \n",
    "        return embedding\n",
    "    \n",
    "    def _get_synonyms(self, word: str) -> List[str]:\n",
    "        \"\"\"Get synonyms for better text analysis\"\"\"\n",
    "        synonyms = {\n",
    "            'space': ['universe', 'cosmos', 'galaxy', 'stars', 'planets', 'void'],\n",
    "            'war': ['battle', 'fight', 'combat', 'conflict', 'warfare', 'violence'],\n",
    "            'rich': ['wealthy', 'elite', 'luxury', 'expensive', 'premium', 'lavish'],\n",
    "            'drinking': ['alcohol', 'beverage', 'wine', 'champagne', 'cocktail'],\n",
    "            'money': ['cash', 'currency', 'profit', 'wealth', 'dollars', 'coins'],\n",
    "            'fire': ['flame', 'burn', 'heat', 'blaze', 'inferno'],\n",
    "            'explosion': ['blast', 'boom', 'burst', 'detonation', 'bang']\n",
    "        }\n",
    "        return synonyms.get(word, [])\n",
    "    \n",
    "    def generate_video(self,\n",
    "                      prompt: str,\n",
    "                      duration_seconds: float = 10.0,\n",
    "                      fps: int = 12,\n",
    "                      resolution: Tuple[int, int] = (512, 512),\n",
    "                      output_filename: Optional[str] = None) -> str:\n",
    "        \"\"\"Generate a video from text prompt\"\"\"\n",
    "        \n",
    "        total_frames = int(duration_seconds * fps)\n",
    "        width, height = resolution\n",
    "        \n",
    "        print(f\"ğŸ¬ Generating {duration_seconds}s video at {fps}FPS ({total_frames} frames)\")\n",
    "        print(f\"ğŸ“ Prompt: '{prompt}'\")\n",
    "        print(f\"ğŸ“º Resolution: {width}x{height}\")\n",
    "        \n",
    "        # Convert text to embedding\n",
    "        text_embedding = self._text_to_embedding(prompt)\n",
    "        \n",
    "        frames = []\n",
    "        \n",
    "        print(\"ğŸ¨ Generating frames...\")\n",
    "        for frame_idx in range(total_frames):\n",
    "            # Create time embedding\n",
    "            time_step = torch.tensor([frame_idx / total_frames], device=self.device)\n",
    "            \n",
    "            # Generate frame\n",
    "            frame_array = np.zeros((height, width, 3), dtype=np.uint8)\n",
    "            \n",
    "            # Generate pixel by pixel (this creates the visual effect)\n",
    "            for y in range(0, height, 4):  # Sample every 4 pixels for speed\n",
    "                for x in range(0, width, 4):\n",
    "                    # Create position embedding\n",
    "                    pos_embed = torch.zeros(507, device=self.device)  # 512 - 3 - 1 - 1 = 507\n",
    "                    pos_embed[0] = x / width\n",
    "                    pos_embed[1] = y / height\n",
    "                    \n",
    "                    # Add some spatial patterns based on text\n",
    "                    if 'space' in prompt.lower():\n",
    "                        # Star field effect\n",
    "                        if np.random.random() < 0.001:  # Sparse stars\n",
    "                            pos_embed[2] = 1.0\n",
    "                    \n",
    "                    if 'war' in prompt.lower():\n",
    "                        # Explosion patterns\n",
    "                        center_x, center_y = width//2, height//2\n",
    "                        dist = np.sqrt((x-center_x)**2 + (y-center_y)**2)\n",
    "                        pos_embed[3] = np.sin(dist * 0.1 + frame_idx * 0.3)\n",
    "                    \n",
    "                    if 'rich' in prompt.lower() or 'drinking' in prompt.lower():\n",
    "                        # Luxury gradient effect  \n",
    "                        pos_embed[4] = np.sin(x * 0.02 + frame_idx * 0.1)\n",
    "                        pos_embed[5] = np.cos(y * 0.02 + frame_idx * 0.1)\n",
    "                    \n",
    "                    # Generate color for this position\n",
    "                    with torch.no_grad():\n",
    "                        color = self.frame_generator(text_embedding, time_step, pos_embed)\n",
    "                        rgb = (color.cpu().numpy() * 255).astype(np.uint8)\n",
    "                    \n",
    "                    # Fill 4x4 block\n",
    "                    frame_array[y:y+4, x:x+4] = rgb\n",
    "            \n",
    "            # Apply some post-processing effects based on content\n",
    "            frame_array = self._apply_effects(frame_array, prompt, frame_idx, total_frames)\n",
    "            \n",
    "            frames.append(Image.fromarray(frame_array))\n",
    "            \n",
    "            # Progress indicator\n",
    "            if (frame_idx + 1) % max(1, total_frames // 10) == 0:\n",
    "                progress = (frame_idx + 1) / total_frames * 100\n",
    "                print(f\"â³ Progress: {progress:.1f}%\")\n",
    "        \n",
    "        print(f\"âœ… Generated {len(frames)} frames\")\n",
    "        \n",
    "        # Save video\n",
    "        video_path = self._save_video(frames, fps, output_filename)\n",
    "        \n",
    "        return video_path\n",
    "    \n",
    "    def _apply_effects(self, frame: np.ndarray, prompt: str, frame_idx: int, total_frames: int) -> np.ndarray:\n",
    "        \"\"\"Apply visual effects based on prompt content\"\"\"\n",
    "        \n",
    "        prompt_lower = prompt.lower()\n",
    "        \n",
    "        # Space effects\n",
    "        if 'space' in prompt_lower:\n",
    "            # Add stars\n",
    "            if np.random.random() < 0.1:\n",
    "                star_x = np.random.randint(0, frame.shape[1])\n",
    "                star_y = np.random.randint(0, frame.shape[0])\n",
    "                frame[star_y-1:star_y+2, star_x-1:star_x+2] = [255, 255, 255]\n",
    "        \n",
    "        # War effects\n",
    "        if 'war' in prompt_lower or 'explosion' in prompt_lower:\n",
    "            # Add flashing effect\n",
    "            if frame_idx % 10 < 3:  # Flash every 10 frames\n",
    "                frame = np.clip(frame.astype(np.float32) * 1.3, 0, 255).astype(np.uint8)\n",
    "        \n",
    "        # Rich/luxury effects\n",
    "        if 'rich' in prompt_lower or 'luxury' in prompt_lower:\n",
    "            # Add golden tint\n",
    "            frame[:, :, 0] = np.clip(frame[:, :, 0] + 20, 0, 255)  # More red\n",
    "            frame[:, :, 1] = np.clip(frame[:, :, 1] + 10, 0, 255)  # More green\n",
    "        \n",
    "        # Drinking/champagne effects\n",
    "        if 'drinking' in prompt_lower or 'champagne' in prompt_lower:\n",
    "            # Add bubble effect\n",
    "            for _ in range(5):\n",
    "                bubble_x = np.random.randint(10, frame.shape[1]-10)\n",
    "                bubble_y = np.random.randint(10, frame.shape[0]-10)\n",
    "                cv2_available = False\n",
    "                try:\n",
    "                    import cv2\n",
    "                    cv2_available = True\n",
    "                except:\n",
    "                    pass\n",
    "                \n",
    "                if cv2_available:\n",
    "                    cv2.circle(frame, (bubble_x, bubble_y), 3, (255, 255, 255), 1)\n",
    "                else:\n",
    "                    # Simple circle without cv2\n",
    "                    frame[bubble_y-2:bubble_y+3, bubble_x-2:bubble_x+3] = [200, 220, 255]\n",
    "        \n",
    "        return frame\n",
    "    \n",
    "    def _save_video(self, frames: List[Image.Image], fps: int, filename: Optional[str] = None) -> str:\n",
    "        \"\"\"Save frames as video file\"\"\"\n",
    "        \n",
    "        if filename is None:\n",
    "            filename = f\"video_{len(list(self.output_dir.glob('*.mp4'))) + 1:04d}.mp4\"\n",
    "        \n",
    "        if not filename.endswith('.mp4'):\n",
    "            filename += '.mp4'\n",
    "            \n",
    "        output_path = self.output_dir / filename\n",
    "        \n",
    "        # Convert PIL images to numpy arrays\n",
    "        frame_arrays = [np.array(frame) for frame in frames]\n",
    "        \n",
    "        try:\n",
    "            # Try to save as MP4\n",
    "            imageio.mimsave(str(output_path), frame_arrays, fps=fps, codec='libx264')\n",
    "            print(f\"ğŸ’¾ Video saved: {output_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸  MP4 save failed: {e}\")\n",
    "            # Save as GIF instead\n",
    "            gif_path = output_path.with_suffix('.gif')\n",
    "            imageio.mimsave(str(gif_path), frame_arrays, fps=fps)\n",
    "            print(f\"ğŸ’¾ Video saved as GIF: {gif_path}\")\n",
    "            output_path = gif_path\n",
    "        \n",
    "        return str(output_path)\n",
    "\n",
    "def get_user_input():\n",
    "    \"\"\"Get user input for video generation\"\"\"\n",
    "    print(\"\\nğŸ¬ Video Generation Setup\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    prompt = input(\"ğŸ“ Enter your video description: \").strip()\n",
    "    if not prompt:\n",
    "        print(\"âŒ Prompt cannot be empty!\")\n",
    "        return None\n",
    "    \n",
    "    # Duration\n",
    "    while True:\n",
    "        try:\n",
    "            duration_input = input(\"â±ï¸  Duration in seconds (5-30, default 10): \").strip()\n",
    "            if not duration_input:\n",
    "                duration = 10.0\n",
    "                break\n",
    "            duration = float(duration_input)\n",
    "            if 5 <= duration <= 30:\n",
    "                break\n",
    "            else:\n",
    "                print(\"âš ï¸  Duration must be between 5 and 30 seconds\")\n",
    "        except ValueError:\n",
    "            print(\"âš ï¸  Please enter a valid number\")\n",
    "    \n",
    "    # Resolution\n",
    "    print(\"\\nğŸ“º Resolution Options:\")\n",
    "    print(\"1. Low (256x256) - Fast generation\")\n",
    "    print(\"2. Medium (512x512) - Balanced\")\n",
    "    print(\"3. High (768x768) - Best quality\")\n",
    "    \n",
    "    while True:\n",
    "        res_choice = input(\"Resolution (1-3, default 2): \").strip() or \"2\"\n",
    "        if res_choice == \"1\":\n",
    "            resolution = (256, 256)\n",
    "            break\n",
    "        elif res_choice == \"2\":\n",
    "            resolution = (512, 512)\n",
    "            break\n",
    "        elif res_choice == \"3\":\n",
    "            resolution = (768, 768)\n",
    "            break\n",
    "        else:\n",
    "            print(\"âš ï¸  Please choose 1, 2, or 3\")\n",
    "    \n",
    "    # FPS\n",
    "    while True:\n",
    "        try:\n",
    "            fps = int(input(\"ğŸï¸  FPS (8-24, default 12): \") or \"12\")\n",
    "            if 8 <= fps <= 24:\n",
    "                break\n",
    "            else:\n",
    "                print(\"âš ï¸  FPS must be between 8 and 24\")\n",
    "        except ValueError:\n",
    "            print(\"âš ï¸  Please enter a valid number\")\n",
    "    \n",
    "    # Filename\n",
    "    filename = input(\"ğŸ’¾ Output filename (optional): \").strip()\n",
    "    if not filename:\n",
    "        filename = None\n",
    "    \n",
    "    return {\n",
    "        'prompt': prompt,\n",
    "        'duration': duration,\n",
    "        'resolution': resolution,\n",
    "        'fps': fps,\n",
    "        'filename': filename\n",
    "    }\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main interactive interface\"\"\"\n",
    "    \n",
    "    print(\"ğŸ¥ Working Text-to-Video Generator\")\n",
    "    print(\"=\" * 50)\n",
    "    print(\"âœ¨ No dependency conflicts - Just works!\")\n",
    "    print()\n",
    "    \n",
    "    # Initialize model\n",
    "    print(\"ğŸ”„ Initializing model...\")\n",
    "    model = WorkingTextToVideo()\n",
    "    \n",
    "    while True:\n",
    "        print(\"\\n\" + \"=\" * 50)\n",
    "        print(\"ğŸ¬ Video Generation Menu\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        choice = input(\"\"\"\n",
    "Choose an option:\n",
    "1. ğŸš€ Generate Video\n",
    "2. ğŸ“ Show Generated Videos  \n",
    "3. âŒ Exit\n",
    "\n",
    "Enter choice (1-3): \"\"\").strip()\n",
    "        \n",
    "        if choice == \"1\":\n",
    "            # Video generation\n",
    "            user_config = get_user_input()\n",
    "            if user_config is None:\n",
    "                continue\n",
    "            \n",
    "            print(f\"\\nğŸ¬ Generation Summary:\")\n",
    "            print(f\"   ğŸ“ Prompt: {user_config['prompt']}\")\n",
    "            print(f\"   â±ï¸  Duration: {user_config['duration']} seconds\")\n",
    "            print(f\"   ğŸ“º Resolution: {user_config['resolution'][0]}x{user_config['resolution'][1]}\")\n",
    "            print(f\"   ğŸï¸  FPS: {user_config['fps']}\")\n",
    "            \n",
    "            confirm = input(\"\\nâœ… Generate video? (y/n): \").strip().lower()\n",
    "            if confirm not in ['y', 'yes']:\n",
    "                print(\"âŒ Generation cancelled\")\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                video_path = model.generate_video(\n",
    "                    prompt=user_config['prompt'],\n",
    "                    duration_seconds=user_config['duration'],\n",
    "                    fps=user_config['fps'],\n",
    "                    resolution=user_config['resolution'],\n",
    "                    output_filename=user_config['filename']\n",
    "                )\n",
    "                print(f\"\\nğŸ‰ Success! Video saved to: {video_path}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"\\nâŒ Generation failed: {e}\")\n",
    "        \n",
    "        elif choice == \"2\":\n",
    "            # Show videos\n",
    "            video_files = list(model.output_dir.glob(\"*.mp4\")) + list(model.output_dir.glob(\"*.gif\"))\n",
    "            \n",
    "            if video_files:\n",
    "                print(f\"\\nğŸ“ Generated Videos ({len(video_files)} files):\")\n",
    "                print(\"-\" * 40)\n",
    "                for i, video_file in enumerate(video_files, 1):\n",
    "                    size_mb = video_file.stat().st_size / (1024 * 1024)\n",
    "                    print(f\"   {i}. {video_file.name} ({size_mb:.1f} MB)\")\n",
    "                print(f\"\\nğŸ“‚ Location: {model.output_dir.absolute()}\")\n",
    "            else:\n",
    "                print(\"\\nğŸ“ No videos generated yet\")\n",
    "        \n",
    "        elif choice == \"3\":\n",
    "            print(\"\\nğŸ‘‹ Thanks for using the Working Text-to-Video Generator!\")\n",
    "            break\n",
    "        else:\n",
    "            print(\"âŒ Invalid choice\")\n",
    "        \n",
    "        input(\"\\nPress Enter to continue...\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0223cca2-83a8-4e69-9633-5485dc938d7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
